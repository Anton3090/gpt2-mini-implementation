{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6XYMLrUlGd4",
        "outputId": "612b40bf-d1a9-4dfd-ca30-61a9eae6080e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Installing collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624,
          "referenced_widgets": [
            "1034715b1b0d4894b752edc7ad35dce6",
            "804c5f0a22fc49d2af01a8f9219e7baa",
            "921b935d9fe040d9bdd9f29a2b53caae",
            "3eaccc250d5e4243a452e1007168af9a",
            "4e372327f083470eb66bd0c6ed2658e6",
            "14fd5567fce74e5289dc9d07489c9767",
            "b28dd3a74456455fb9d1b8344d2340f5",
            "171b05faf71a4857bd52805ed810cdcc",
            "9bb476c81629445baf416423d51aac69",
            "9bcb3d25c43e42fb8bccca81bf55ab7f",
            "f00a7debb42e4f8885c9f819671f412c",
            "9a984d1d5fc9423c817325a00b9f2317",
            "d4f00103c3b84414af5d2d3cf73e3c52",
            "917507ace79b48e18a4588c01e7f21aa",
            "1c17cc1d136a4790a6a1fc03fb30f97f",
            "9ee18f2f058148bebcfc786cc55801bf",
            "f53befcbcd8542d7aa61fb3d2264c9b9",
            "45eeb07d9f4a4f8f83c251de97eaf2b4",
            "01d15fd25e134171bafe8ad04f70aef0",
            "b51fdb2e932c4fd9b7174db0f2c47e76",
            "bed2a98a10dc4b92acc96a60bb5765b7",
            "ba00fc8113dc44b4bebedb7a73eaca17",
            "8a6be4e49e094072a2a0b9e69456538e",
            "9d80aa5c297d4483b5baed1d10ea804e",
            "ea0e3c2777e949889b26e06a29401db0",
            "9159c9944a07417e9a953802b1aaa766",
            "c0750ede88e74b0dbef36b4e64b0738a",
            "9339a1dedf584c07b9ad7e63daaab650",
            "a86a4577fc814e318c69d4c341f1ac52",
            "cab47bec0cf94d6dbbd78ec090d5073f",
            "deda1cf33c8a4b2389b07c5e2d304545",
            "446aa391ff3d477282fbfbfe20b90c1e",
            "5dd4a686c6d1483b96fae736eab4788c",
            "e8dce456243841708101da9d446781ae",
            "9ce1abe7e917440fb8fbf93ed3b6ec51",
            "666e9a32d8564a31b02cf76d829d4f4c",
            "1fbda617ed4f4f798dc39b23ece35469",
            "0641475a4c4b41d3965177e3dbb99cd9",
            "64e6d1d32b74447f9ff2d3ecffce7a7d",
            "6e98e8426cea439fb7c2adac933943c3",
            "cb3b91bd8f2d4e9bbeeff0401fb3c285",
            "0beb8d4ba1e24f71aebcad8df350ba8c",
            "164d930f77594ec28c8ca8eec01888e4",
            "d7cfd2338a494e56b5de96503b1a3305",
            "3b901df594bb4fb3b1d033000a7734d0",
            "be0ea611a19b4889a934af9530d6f165",
            "db816c69bacc460fbba53ab15e2c6cb6",
            "e12fde6f58e34fd497bdcd3e02cf4386",
            "ea3523702fb7454c92ed601869b7a94c",
            "51279e7ba93f4e2ca7aaa801c0bbcbfc",
            "f50aae1c2c8d46e0b6b8d6eea3d3c0c3",
            "85ea60d6880a47c9b387bf2138ed7641",
            "8a9040e535d44e0ea7dd6f3ae483e6c9",
            "f52c3f9f51eb4c238297a3b534df84c3",
            "4f6cfd6b2fe34db788d1493347e7c0a3",
            "5e682d95c1a644f4a2bea920444dcf22",
            "10d0afd7bdf74f128242a4f69adbbf50",
            "94450b6a5a4c4075a071205c433c0611",
            "1fe6043470cf4bfbab1998174378872d",
            "dc152a9fee9a42f98623b27dd640537b",
            "d19e5bee4ee0427eb092df31e94a1707",
            "dbcdc192bdbf4191af817273bb06ee46",
            "a2eead760714494695df8a9f2603c85d",
            "504d6904212640f0b9dca0a90a570fbe",
            "c11a4295fe6b41919851e7ccd7b92041",
            "f907ef803f2c4ffabcbcf4c144ae9192",
            "c5e6f014ac37497c840cb851f52ac2c4",
            "8f15b2af14524fe295783a1a730b3c41",
            "4c4c36d48adb48e88d08bd4ac1b4ab1a",
            "10245df7361343239354724ac31bdc46",
            "c4a6cd7fb2bf48939a5d3dd2f2b6eba7",
            "5905d60d77b540f580cb2d64e4ac2a99",
            "42ec04b69206415183bde4c7dc6a2ea5",
            "a10b44b663894b7d942d8556f3b076be",
            "9101cc79be5d4fa895c09e912ad10ea9",
            "b9df927e44704301925aac4250bc6127",
            "6ce52c85fad449a8a74bfdc15846154c",
            "1659fbe466454db2abbb00e2bf28e7e8",
            "c047786c0f614efebe4050de52b7a138",
            "5254e7d98f65491c9311f71c44a61973",
            "51d765c21f5b488c9857f7f5be26d033",
            "cf01b180f86c42498d9e3ba1c4513811",
            "5f89650129ef4bd8917a7be4e5902958",
            "e0cd57e4e0584cdab8d9e0d614fa5f9a",
            "29446fb409994e8989c336473b641f9c",
            "addd36c935864167bee6f5a899e52b34",
            "8b07e58000dd4f5ea009aed06dcf7af2",
            "97ab9af6bc814adebe79cb41aa48e774"
          ]
        },
        "id": "c2b8GkvqlfRV",
        "outputId": "a9f9e8be-ed60-4a28-89d1-08d97133fcc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1034715b1b0d4894b752edc7ad35dce6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a984d1d5fc9423c817325a00b9f2317",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00000-of-00004-2d5a1467fff1081b.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a6be4e49e094072a2a0b9e69456538e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00001-of-00004-5852b56a2bd28fd9.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8dce456243841708101da9d446781ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00002-of-00004-a26307300439e943.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b901df594bb4fb3b1d033000a7734d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00003-of-00004-d243063613e5a057.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e682d95c1a644f4a2bea920444dcf22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00000-of-00001-869c898b519ad725.parquet:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5e6f014ac37497c840cb851f52ac2c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1659fbe466454db2abbb00e2bf28e7e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
            "\n",
            "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
            "\n",
            "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#  Correct approach\n",
        "dataset = load_dataset(\n",
        "    \"roneneldan/TinyStories\",\n",
        "    split=\"train\"  # Use \"default\" config implicitly\n",
        ")\n",
        "\n",
        "# Verify the dataset\n",
        "print(dataset[0][\"text\"])  # Should output a story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Qyz44Zls7r",
        "outputId": "578e9ba6-4f65-4800-808b-2453aa2583ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive  # Required in every new session\n",
        "from datasets import load_from_disk\n",
        "\n",
        "drive.mount('/content/drive')  # Re-authenticate (tiny bandwidth)\n",
        "dataset = load_from_disk(\"/content/drive/MyDrive/TinyStories\")  # No internet needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7lDoQol4Na",
        "outputId": "c656570e-d868-42ff-9a94-1e4a8b243670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "After restarting session:\n",
            "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with...\n"
          ]
        }
      ],
      "source": [
        "# 1. Mount Drive again (NEW SESSION)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Load dataset from Drive\n",
        "from datasets import load_from_disk\n",
        "dataset = load_from_disk(\"/content/drive/MyDrive/TinyStories\")\n",
        "\n",
        "# 3. Print the SAME story (NO INTERNET REQUIRED)\n",
        "print(\"After restarting session:\")\n",
        "print(dataset[0][\"text\"][:100] + \"...\")  # Same partial text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2h2Y1G9l6Vb"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from datasets import load_from_disk\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "357y6hQXmArf",
        "outputId": "fb16140f-545c-4be3-8ef8-9b3d4c3abf4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qk7n6qhmCb-"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "torch.manual_seed(1337)\n",
        "# 2. Load only 75% of the dataset\n",
        "n = int(0.75 * len(dataset))\n",
        "dataset = dataset.select(range(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUKPtCeNmHhZ"
      },
      "outputs": [],
      "source": [
        "# 3. Concatenate and Tokenize\n",
        "text = \"\".join(dataset[i][\"text\"] for i in range(len(dataset)))\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyhl6io9miuG"
      },
      "outputs": [],
      "source": [
        "# 4. Encode and save in chunks\n",
        "output_dir = \"/content/encoded_chunks\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "chunk_size = 1000\n",
        "counter = 0\n",
        "for i in range(0, len(dataset), chunk_size):\n",
        "    chunk = [dataset[j][\"text\"] for j in range(i, min(i + chunk_size, len(dataset)))]\n",
        "    encoded = [encode(text) for text in chunk]\n",
        "    flat = [item for sublist in encoded for item in sublist]\n",
        "    chunk_tensor = torch.tensor(flat, dtype=torch.int32)\n",
        "    torch.save(chunk_tensor, os.path.join(output_dir, f\"chunk_{counter}.pt\"))\n",
        "    counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixxGyZmNm50D"
      },
      "outputs": [],
      "source": [
        "# 5. Combine into single binary file\n",
        "final_output_path = \"/content/full_encoded_data.bin\"\n",
        "with open(final_output_path, 'wb') as f:\n",
        "    chunk_paths = sorted(glob(os.path.join(output_dir, \"chunk_*.pt\")))\n",
        "    total_tokens = 0\n",
        "    for path in chunk_paths:\n",
        "        chunk = torch.load(path)\n",
        "        f.write(chunk.numpy().astype('int32').tobytes())\n",
        "        total_tokens += chunk.numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjuleUX0m7ni"
      },
      "outputs": [],
      "source": [
        "# 6. Split into train/val\n",
        "split_point = int(0.9 * total_tokens)\n",
        "train_path = \"/content/train_data.bin\"\n",
        "val_path = \"/content/val_data.bin\"\n",
        "with open(final_output_path, 'rb') as f:\n",
        "    train_bytes = f.read(split_point * 4)\n",
        "    with open(train_path, 'wb') as train_file:\n",
        "        train_file.write(train_bytes)\n",
        "    val_bytes = f.read()\n",
        "    with open(val_path, 'wb') as val_file:\n",
        "        val_file.write(val_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_4U-zHZnDYi"
      },
      "outputs": [],
      "source": [
        "# 7. Memmap train/val\n",
        "train_data = np.memmap(train_path, dtype=np.int32, mode='r')\n",
        "val_data = np.memmap(val_path, dtype=np.int32, mode='r')\n",
        "\n",
        "# 8. Data loader\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([torch.tensor(data[i:i+block_size], dtype=torch.long) for i in ix])\n",
        "    y = torch.stack([torch.tensor(data[i+1:i+block_size+1], dtype=torch.long) for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJof-YHAnLLy"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OoVuuj8nVxu"
      },
      "outputs": [],
      "source": [
        "# 9. Model definition\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVjuhGjyneBJ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.proj(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8467qtjnhnJ"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj9LOaLvnlvR"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36fWMB_InpT4"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.view(B*T, -1)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37wxsQ0gn2He",
        "outputId": "5b9ad57a-a6ee-423a-f36d-0f8c331e8494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.223145 M parameters\n",
            "step 0: train loss 5.2577, val loss 5.2626\n",
            "step 100: train loss 2.5419, val loss 2.5393\n",
            "step 200: train loss 2.3581, val loss 2.3554\n",
            "step 300: train loss 2.2157, val loss 2.2122\n",
            "step 400: train loss 2.1300, val loss 2.1413\n",
            "step 500: train loss 2.0874, val loss 2.0713\n",
            "step 600: train loss 1.9904, val loss 1.9936\n",
            "step 700: train loss 1.9360, val loss 1.9273\n",
            "step 800: train loss 1.8755, val loss 1.8780\n",
            "step 900: train loss 1.8152, val loss 1.8220\n",
            "step 1000: train loss 1.7739, val loss 1.7722\n",
            "step 1100: train loss 1.7369, val loss 1.7313\n",
            "step 1200: train loss 1.7066, val loss 1.7032\n",
            "step 1300: train loss 1.6844, val loss 1.6832\n",
            "step 1400: train loss 1.6615, val loss 1.6601\n",
            "step 1500: train loss 1.6206, val loss 1.6223\n",
            "step 1600: train loss 1.5997, val loss 1.6038\n",
            "step 1700: train loss 1.5920, val loss 1.5854\n",
            "step 1800: train loss 1.5783, val loss 1.5812\n",
            "step 1900: train loss 1.5610, val loss 1.5676\n",
            "step 2000: train loss 1.5425, val loss 1.5488\n",
            "step 2100: train loss 1.5370, val loss 1.5441\n",
            "step 2200: train loss 1.5199, val loss 1.5257\n",
            "step 2300: train loss 1.5143, val loss 1.5077\n",
            "step 2400: train loss 1.4950, val loss 1.4937\n",
            "step 2500: train loss 1.4858, val loss 1.4811\n",
            "step 2600: train loss 1.4719, val loss 1.4633\n",
            "step 2700: train loss 1.4691, val loss 1.4636\n",
            "step 2800: train loss 1.4644, val loss 1.4585\n",
            "step 2900: train loss 1.4425, val loss 1.4458\n",
            "step 3000: train loss 1.4425, val loss 1.4309\n",
            "step 3100: train loss 1.4370, val loss 1.4470\n",
            "step 3200: train loss 1.4225, val loss 1.4157\n",
            "step 3300: train loss 1.4164, val loss 1.4251\n",
            "step 3400: train loss 1.4130, val loss 1.4113\n",
            "step 3500: train loss 1.4042, val loss 1.4095\n",
            "step 3600: train loss 1.4115, val loss 1.4129\n",
            "step 3700: train loss 1.3925, val loss 1.3834\n",
            "step 3800: train loss 1.3956, val loss 1.3929\n",
            "step 3900: train loss 1.3913, val loss 1.3807\n",
            "step 4000: train loss 1.3764, val loss 1.3766\n",
            "step 4100: train loss 1.3752, val loss 1.3718\n",
            "step 4200: train loss 1.3697, val loss 1.3804\n",
            "step 4300: train loss 1.3715, val loss 1.3721\n",
            "step 4400: train loss 1.3626, val loss 1.3675\n",
            "step 4500: train loss 1.3535, val loss 1.3710\n",
            "step 4600: train loss 1.3578, val loss 1.3586\n",
            "step 4700: train loss 1.3526, val loss 1.3517\n",
            "step 4800: train loss 1.3503, val loss 1.3543\n",
            "step 4900: train loss 1.3453, val loss 1.3412\n",
            "step 5000: train loss 1.3413, val loss 1.3389\n",
            "step 5100: train loss 1.3251, val loss 1.3325\n",
            "step 5200: train loss 1.3377, val loss 1.3246\n",
            "step 5300: train loss 1.3338, val loss 1.3274\n",
            "step 5400: train loss 1.3257, val loss 1.3310\n",
            "step 5500: train loss 1.3263, val loss 1.3296\n",
            "step 5600: train loss 1.3219, val loss 1.3125\n",
            "step 5700: train loss 1.3262, val loss 1.3174\n",
            "step 5800: train loss 1.3242, val loss 1.3185\n",
            "step 5900: train loss 1.3214, val loss 1.3182\n",
            "step 6000: train loss 1.3110, val loss 1.3088\n",
            "step 6100: train loss 1.3175, val loss 1.3033\n",
            "step 6200: train loss 1.3102, val loss 1.2942\n",
            "step 6300: train loss 1.3136, val loss 1.2959\n",
            "step 6400: train loss 1.2993, val loss 1.3079\n",
            "step 6500: train loss 1.3012, val loss 1.3018\n",
            "step 6600: train loss 1.2993, val loss 1.2911\n",
            "step 6700: train loss 1.2856, val loss 1.2858\n",
            "step 6800: train loss 1.3040, val loss 1.2959\n",
            "step 6900: train loss 1.2834, val loss 1.2991\n",
            "step 7000: train loss 1.2760, val loss 1.2822\n",
            "step 7100: train loss 1.2929, val loss 1.3046\n",
            "step 7200: train loss 1.2903, val loss 1.2959\n",
            "step 7300: train loss 1.2959, val loss 1.2889\n",
            "step 7400: train loss 1.2734, val loss 1.2717\n",
            "step 7500: train loss 1.2763, val loss 1.2752\n",
            "step 7600: train loss 1.2926, val loss 1.2891\n",
            "step 7700: train loss 1.2895, val loss 1.2872\n",
            "step 7800: train loss 1.2830, val loss 1.2903\n",
            "step 7900: train loss 1.2737, val loss 1.2607\n",
            "step 8000: train loss 1.2683, val loss 1.2762\n",
            "step 8100: train loss 1.2661, val loss 1.2726\n",
            "step 8200: train loss 1.2528, val loss 1.2627\n",
            "step 8300: train loss 1.2687, val loss 1.2743\n",
            "step 8400: train loss 1.2710, val loss 1.2692\n",
            "step 8500: train loss 1.2704, val loss 1.2585\n",
            "step 8600: train loss 1.2607, val loss 1.2639\n",
            "step 8700: train loss 1.2602, val loss 1.2713\n",
            "step 8800: train loss 1.2631, val loss 1.2460\n",
            "step 8900: train loss 1.2531, val loss 1.2607\n",
            "step 9000: train loss 1.2554, val loss 1.2535\n",
            "step 9100: train loss 1.2575, val loss 1.2461\n",
            "step 9200: train loss 1.2694, val loss 1.2572\n",
            "step 9300: train loss 1.2504, val loss 1.2528\n",
            "step 9400: train loss 1.2525, val loss 1.2523\n",
            "step 9500: train loss 1.2506, val loss 1.2506\n",
            "step 9600: train loss 1.2333, val loss 1.2431\n",
            "step 9700: train loss 1.2472, val loss 1.2536\n",
            "step 9800: train loss 1.2526, val loss 1.2495\n",
            "step 9900: train loss 1.2481, val loss 1.2508\n"
          ]
        }
      ],
      "source": [
        "# 10. Instantiate model\n",
        "model = BigramLanguageModel().to(device)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# 11. Training loop\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7-LjqlFt0D7",
        "outputId": "5fbd6cfa-2c38-45a5-de2d-a6f76f0c8330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tlet oway.Mom said the comes off treeze.\"\"\n",
            "\n",
            "On. Lily didn't very having.Lily been?\"\n",
            "\n",
            "Her smiled and sece creat it's chate for Lily. They everyone that day, she the veasitly with he couldn't want to not things. The rain named Lily.\n",
            "\n",
            "When you have to threw are renched, notwies watchester.\n",
            "\n",
            "One day, he had a little britch found silong.\n",
            "\n",
            "The little girl named Lucy laugh oversed out he the next for sweet\".Sara was scared agail wagily. She ammazed and said the big, and went to the old carebratuat!\" The triestingere the little bird who drived and take she a servone. She racted to shape. She circlents okay, from old pretty! Mommy scelfed seen happy. Sam so had she peeced in the reusurplach happy. She had so much as scare?\"\n",
            "\n",
            "\n",
            "Jack fencrous it handed and crastun felt better. Everywhere she felt un was very for or playing with Sara. So You showed her sit finisheds.\n",
            "She liked to when the best many. She tri€™ted were relievs deady! Everyoney, but streezed and anumages.Once upon a time, there was appy picked out out cozych other climb. But the friends, goidn's facebwins of to help the box. He went to faster. It was gave some full with at the key.\n",
            "\n",
            "Nather smell the girl gave them to play time, a night sisting in the sky. Tom would make scared the chozs liked being to a forest. They acand polish it dog not lives.\n",
            "\n",
            "Wigh playod with a sound feels that like friend. They had and use not them for laughs and driverfuz: She didn't said them. \n",
            "\n",
            "The nevery day. You knew it a gave.\"\n",
            "\n",
            "Salm. They fin the stickeshper, making themer back. His mom back and mhappy shiny and yummy went to the string. They are three happy. He rived side. She sees a boy whiles be elac. \n",
            "\n",
            "The first day and Mom. Stugged and she lived out fun! It play with little spion.\n",
            "\n",
            "Everying asking them hoppy seen the slide small to Dad.Once upon a tight,\" she pulled and pought again.\n",
            "\n",
            "He sailed she had to reached a ringllow or and fall. The boy said, \"OK, The bird, door nothis too the friends. She shafe she use my ran. You came wear and telep! She pinked them. He placed to monk her mom stopped at the birt decorn to his other. Lily did now sharion to make what a diciced her new, his friends and put the honest in enjt. He felt and started to do. His hand and the drees szaid and special first. She did not suruch anything is cake. It was unished frox when she for makie a bike very her.Once upon a time there streat to clerct twich bud coming. They go you. She says. EHowes a chooker.\"\n",
            "\n",
            "Hurried asking.\n",
            "\n",
            "This light to perplics the drect soble. They also regust underful notice and hem, she colous should pend where obfulce! Let's friends from she was touch angry. She said, \"OK day did not, Jenny times He had to eavy. She had someome tplasar, and do. He could anymants. \n",
            "\n",
            "The next apon a time, there was a time tday!\n",
            "\n",
            "Max went to be careful so prove flew uf he to put someone the garden. Come of lef. So Jack were haty in the what happen found a parker to the wish their kind.\n",
            "\n",
            "The little next.\n",
            "\n",
            "\n",
            "\"Don't on erploce anyor. I asked his momma some or much! \n",
            "\n",
            "The trod to shook mame went. He found so impor the bird and as sharies, but she new being and wanted to stake to the swimming. She wanted together. He knew shalwisped and havelor. \n",
            "\n",
            "Everythis! ut red the flew their mom. \n",
            "\n",
            "\n",
            "When helps On he dangeam.\"\n",
            "\n",
            "\"Give was my said,\" Banny said, \"Plus were not steak! It was imprants and like sound wish like a mom! One day, he didn't want chasted and said \"Don't cat have the veryn.\n",
            "\n",
            "The two there went hoding up the lives. They played with with some if it didnâ€Once upon a time. He can do tried and him first a funce.\n",
            "\n",
            "\"Chonieys, named his room that sharing\".\n",
            "\n",
            "The bird smiled and wish maness! She nodged turger.One day, He wanted to veryful fried and said tubs a gain can the got to stay. \n",
            "\n",
            "On the ansted!\"\n",
            "\n",
            "The build.\" \n",
            "\n",
            "Little yelloa money.\n",
            "\n",
            "One day, Every day, Lily had a and licky. Can you can't ready her dearches. The busy was jush. Itâ€\n",
            "\n",
            "Every day was thated to make something you play.\" Tim pant. \n",
            "\n",
            "The ellove perplited to throught. There was fround brave for playing together. They thought it open inteâ€™s would would playing out!\" Lily said. His mom did not catch open the bird. She was placed on his mess! Thank you door your friends.\n",
            "\n",
            "Tom and Ben cake you, the cquiterfuc made happy liked. He and Ben kept for the planter went so being. He gave the laught frown the horst. \n",
            "\n",
            "They had nlew back for dirt.Took becallisted, she kind its.\n",
            "\n",
            "The dipped yellow is nice sunsy-gave she the lived, them and said the more. \n",
            "\n",
            "He has fun. They even him finds on it now finalls. \n",
            "Suddenly weart does nothing never was xamily.\n",
            "\n",
            "Now this friends ar, but the shooking some.\n",
            "\n",
            "The nice stayed to comath the shop. \n",
            "\n",
            "Sust felt lourn and the pliter. They graght say, the seatch frieere. They are playing on, Annay wanted to how found go wilk.\n",
            "\n",
            "So, but he could got mean do playing wist his very narkighty. On the listen streat other quies. He wanted to go favoritfully.\n",
            "\n",
            "What is importated. She said, \"Thank you found a big voice. But their\n"
          ]
        }
      ],
      "source": [
        "# 12. Generate text\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=5000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model weights\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "\n",
        "# Save evaluation metrics and vocabulary\n",
        "import json\n",
        "metrics = {\n",
        "    'train_loss': losses['train'],\n",
        "    'val_loss': losses['val'],\n",
        "    'vocab_size': vocab_size,\n",
        "    'chars': chars,\n",
        "    'stoi': stoi,\n",
        "    'itos': itos\n",
        "}\n",
        "with open('model_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f)\n",
        "\n",
        "print(\"Model weights and metrics saved successfully.\")"
      ],
      "metadata": {
        "id": "WC82PLzFRUm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model weights and metrics (example for future use)\n",
        "# To load in future sessions:\n",
        "model = BigramLanguageModel().to(device)\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "\n",
        "with open('model_metrics.json', 'r') as f:\n",
        "    metrics = json.load(f)\n",
        "    stoi = metrics['stoi']\n",
        "    itos = metrics['itos']\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "uBfCRoLxRfQ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}