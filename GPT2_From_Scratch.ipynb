{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6XYMLrUlGd4",
        "outputId": "05906ba8-0d8e-4ba1-c8c8-b38d105ccc68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2b8GkvqlfRV",
        "outputId": "158dfe08-89ee-4a8c-8846-136b9b956d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
            "\n",
            "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
            "\n",
            "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#  Correct approach\n",
        "dataset = load_dataset(\n",
        "    \"roneneldan/TinyStories\",\n",
        "    split=\"train\"  # Use \"default\" config implicitly\n",
        ")\n",
        "\n",
        "# Verify the dataset\n",
        "print(dataset[0][\"text\"])  # Should output a story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Qyz44Zls7r",
        "outputId": "bb7056f4-d9b2-494e-d0e6-9b1e94d6c90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive  # Required in every new session\n",
        "from datasets import load_from_disk\n",
        "\n",
        "drive.mount('/content/drive')  # Re-authenticate (tiny bandwidth)\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")  # Smaller subset for testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7lDoQol4Na",
        "outputId": "1b8d8d10-5261-474a-ba37-2afa998563d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After restarting session:\n",
            "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  Display the first story sample (NO INTERNET REQUIRED)\n",
        "print(\"After restarting session:\")\n",
        "print(dataset[0][\"text\"][:100] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T2h2Y1G9l6Vb"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from datasets import load_from_disk\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "357y6hQXmArf",
        "outputId": "d24c6308-1939-409c-a405-67747c50dc65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2qk7n6qhmCb-"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "torch.manual_seed(1337)\n",
        "# 2. Load only 75% of the dataset\n",
        "n = int(0.75 * len(dataset))\n",
        "dataset = dataset.select(range(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oUKPtCeNmHhZ"
      },
      "outputs": [],
      "source": [
        "# 3. Concatenate and Tokenize\n",
        "text = \"\".join(dataset[i][\"text\"] for i in range(len(dataset)))\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gyhl6io9miuG"
      },
      "outputs": [],
      "source": [
        "# 4. Encode and save in chunks\n",
        "output_dir = \"/content/encoded_chunks\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "chunk_size = 1000\n",
        "counter = 0\n",
        "for i in range(0, len(dataset), chunk_size):\n",
        "    chunk = [dataset[j][\"text\"] for j in range(i, min(i + chunk_size, len(dataset)))]\n",
        "    encoded = [encode(text) for text in chunk]\n",
        "    flat = [item for sublist in encoded for item in sublist]\n",
        "    chunk_tensor = torch.tensor(flat, dtype=torch.int32)\n",
        "    torch.save(chunk_tensor, os.path.join(output_dir, f\"chunk_{counter}.pt\"))\n",
        "    counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ixxGyZmNm50D"
      },
      "outputs": [],
      "source": [
        "# 5. Combine into single binary file\n",
        "final_output_path = \"/content/full_encoded_data.bin\"\n",
        "with open(final_output_path, 'wb') as f:\n",
        "    chunk_paths = sorted(glob(os.path.join(output_dir, \"chunk_*.pt\")))\n",
        "    total_tokens = 0\n",
        "    for path in chunk_paths:\n",
        "        chunk = torch.load(path)\n",
        "        f.write(chunk.numpy().astype('int32').tobytes())\n",
        "        total_tokens += chunk.numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WjuleUX0m7ni"
      },
      "outputs": [],
      "source": [
        "# 6. Split into train/val\n",
        "split_point = int(0.9 * total_tokens)\n",
        "train_path = \"/content/train_data.bin\"\n",
        "val_path = \"/content/val_data.bin\"\n",
        "with open(final_output_path, 'rb') as f:\n",
        "    train_bytes = f.read(split_point * 4)\n",
        "    with open(train_path, 'wb') as train_file:\n",
        "        train_file.write(train_bytes)\n",
        "    val_bytes = f.read()\n",
        "    with open(val_path, 'wb') as val_file:\n",
        "        val_file.write(val_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "W_4U-zHZnDYi"
      },
      "outputs": [],
      "source": [
        "# 7. Memmap train/val\n",
        "train_data = np.memmap(train_path, dtype=np.int32, mode='r')\n",
        "val_data = np.memmap(val_path, dtype=np.int32, mode='r')\n",
        "\n",
        "# 8. Data loader\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([torch.tensor(data[i:i+block_size], dtype=torch.long) for i in ix])\n",
        "    y = torch.stack([torch.tensor(data[i+1:i+block_size+1], dtype=torch.long) for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kJof-YHAnLLy"
      },
      "outputs": [],
      "source": [
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # Add no_grad context here\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            perplexities = torch.zeros(eval_iters)  # Store perplexities\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "                # Calculate perplexity (exponential of the loss)\n",
        "                perplexities[k] = torch.exp(torch.tensor(loss.item()))\n",
        "            out[split] = {\n",
        "                'loss': losses.mean(),\n",
        "                'perplexity': perplexities.mean()  # Add perplexity to output\n",
        "            }\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2OoVuuj8nVxu"
      },
      "outputs": [],
      "source": [
        "# 9. Model definition\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bVjuhGjyneBJ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.proj(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i8467qtjnhnJ"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Vj9LOaLvnlvR"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "36fWMB_InpT4"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.view(B*T, -1)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37wxsQ0gn2He",
        "outputId": "dfdc6839-e433-4744-9d16-b96d1ff021b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.223145 M parameters\n",
            "step 0: train loss 5.2347, val loss 5.2343\n",
            "step 0: train ppl 187.75, val ppl 187.67\n",
            "step 100: train loss 2.5309, val loss 2.5169\n",
            "step 100: train ppl 12.60, val ppl 12.41\n",
            "step 200: train loss 2.3241, val loss 2.3244\n",
            "step 200: train ppl 10.24, val ppl 10.24\n",
            "step 300: train loss 2.2350, val loss 2.2309\n",
            "step 300: train ppl 9.36, val ppl 9.32\n",
            "step 400: train loss 2.1414, val loss 2.1367\n",
            "step 400: train ppl 8.53, val ppl 8.49\n",
            "step 500: train loss 2.0628, val loss 2.0582\n",
            "step 500: train ppl 7.88, val ppl 7.85\n",
            "step 600: train loss 1.9853, val loss 1.9873\n",
            "step 600: train ppl 7.30, val ppl 7.31\n",
            "step 700: train loss 1.9358, val loss 1.9362\n",
            "step 700: train ppl 6.95, val ppl 6.95\n",
            "step 800: train loss 1.8485, val loss 1.8469\n",
            "step 800: train ppl 6.37, val ppl 6.36\n",
            "step 900: train loss 1.8145, val loss 1.8208\n",
            "step 900: train ppl 6.16, val ppl 6.20\n",
            "step 1000: train loss 1.7654, val loss 1.7644\n",
            "step 1000: train ppl 5.86, val ppl 5.85\n",
            "step 1100: train loss 1.7295, val loss 1.7261\n",
            "step 1100: train ppl 5.65, val ppl 5.64\n",
            "step 1200: train loss 1.7111, val loss 1.6953\n",
            "step 1200: train ppl 5.55, val ppl 5.46\n",
            "step 1300: train loss 1.6666, val loss 1.6605\n",
            "step 1300: train ppl 5.31, val ppl 5.28\n",
            "step 1400: train loss 1.6407, val loss 1.6419\n",
            "step 1400: train ppl 5.17, val ppl 5.18\n",
            "step 1500: train loss 1.6103, val loss 1.6175\n",
            "step 1500: train ppl 5.02, val ppl 5.06\n",
            "step 1600: train loss 1.6146, val loss 1.6041\n",
            "step 1600: train ppl 5.04, val ppl 4.99\n",
            "step 1700: train loss 1.5958, val loss 1.6079\n",
            "step 1700: train ppl 4.95, val ppl 5.01\n",
            "step 1800: train loss 1.5766, val loss 1.5677\n",
            "step 1800: train ppl 4.85, val ppl 4.81\n",
            "step 1900: train loss 1.5635, val loss 1.5547\n",
            "step 1900: train ppl 4.79, val ppl 4.75\n",
            "step 2000: train loss 1.5357, val loss 1.5379\n",
            "step 2000: train ppl 4.66, val ppl 4.67\n",
            "step 2100: train loss 1.5277, val loss 1.5191\n",
            "step 2100: train ppl 4.62, val ppl 4.59\n",
            "step 2200: train loss 1.5062, val loss 1.5177\n",
            "step 2200: train ppl 4.52, val ppl 4.58\n",
            "step 2300: train loss 1.5032, val loss 1.4959\n",
            "step 2300: train ppl 4.51, val ppl 4.48\n",
            "step 2400: train loss 1.4817, val loss 1.4979\n",
            "step 2400: train ppl 4.42, val ppl 4.49\n",
            "step 2500: train loss 1.4767, val loss 1.4708\n",
            "step 2500: train ppl 4.39, val ppl 4.37\n",
            "step 2600: train loss 1.4650, val loss 1.4824\n",
            "step 2600: train ppl 4.34, val ppl 4.42\n",
            "step 2700: train loss 1.4645, val loss 1.4607\n",
            "step 2700: train ppl 4.34, val ppl 4.32\n",
            "step 2800: train loss 1.4648, val loss 1.4511\n",
            "step 2800: train ppl 4.34, val ppl 4.28\n",
            "step 2900: train loss 1.4486, val loss 1.4387\n",
            "step 2900: train ppl 4.27, val ppl 4.23\n",
            "step 3000: train loss 1.4418, val loss 1.4374\n",
            "step 3000: train ppl 4.24, val ppl 4.23\n",
            "step 3100: train loss 1.4338, val loss 1.4360\n",
            "step 3100: train ppl 4.21, val ppl 4.22\n",
            "step 3200: train loss 1.4233, val loss 1.4334\n",
            "step 3200: train ppl 4.16, val ppl 4.21\n",
            "step 3300: train loss 1.4201, val loss 1.4247\n",
            "step 3300: train ppl 4.15, val ppl 4.17\n",
            "step 3400: train loss 1.4204, val loss 1.4199\n",
            "step 3400: train ppl 4.15, val ppl 4.15\n",
            "step 3500: train loss 1.3990, val loss 1.3887\n",
            "step 3500: train ppl 4.07, val ppl 4.02\n",
            "step 3600: train loss 1.4217, val loss 1.4159\n",
            "step 3600: train ppl 4.16, val ppl 4.14\n",
            "step 3700: train loss 1.4007, val loss 1.4045\n",
            "step 3700: train ppl 4.07, val ppl 4.09\n",
            "step 3800: train loss 1.3976, val loss 1.3873\n",
            "step 3800: train ppl 4.06, val ppl 4.02\n",
            "step 3900: train loss 1.3901, val loss 1.3892\n",
            "step 3900: train ppl 4.03, val ppl 4.02\n",
            "step 4000: train loss 1.3778, val loss 1.3820\n",
            "step 4000: train ppl 3.98, val ppl 4.00\n",
            "step 4100: train loss 1.3754, val loss 1.3847\n",
            "step 4100: train ppl 3.97, val ppl 4.01\n",
            "step 4200: train loss 1.3746, val loss 1.3863\n",
            "step 4200: train ppl 3.97, val ppl 4.01\n",
            "step 4300: train loss 1.3722, val loss 1.3748\n",
            "step 4300: train ppl 3.96, val ppl 3.97\n",
            "step 4400: train loss 1.3657, val loss 1.3654\n",
            "step 4400: train ppl 3.93, val ppl 3.93\n",
            "step 4500: train loss 1.3706, val loss 1.3786\n",
            "step 4500: train ppl 3.95, val ppl 3.98\n",
            "step 4600: train loss 1.3503, val loss 1.3680\n",
            "step 4600: train ppl 3.87, val ppl 3.94\n",
            "step 4700: train loss 1.3601, val loss 1.3482\n",
            "step 4700: train ppl 3.91, val ppl 3.86\n",
            "step 4800: train loss 1.3515, val loss 1.3488\n",
            "step 4800: train ppl 3.88, val ppl 3.86\n",
            "step 4900: train loss 1.3467, val loss 1.3446\n",
            "step 4900: train ppl 3.86, val ppl 3.85\n",
            "step 5000: train loss 1.3486, val loss 1.3500\n",
            "step 5000: train ppl 3.87, val ppl 3.87\n",
            "step 5100: train loss 1.3549, val loss 1.3521\n",
            "step 5100: train ppl 3.89, val ppl 3.88\n",
            "step 5200: train loss 1.3391, val loss 1.3470\n",
            "step 5200: train ppl 3.83, val ppl 3.86\n",
            "step 5300: train loss 1.3406, val loss 1.3476\n",
            "step 5300: train ppl 3.83, val ppl 3.86\n",
            "step 5400: train loss 1.3295, val loss 1.3212\n",
            "step 5400: train ppl 3.79, val ppl 3.76\n",
            "step 5500: train loss 1.3392, val loss 1.3256\n",
            "step 5500: train ppl 3.83, val ppl 3.78\n",
            "step 5600: train loss 1.3194, val loss 1.3269\n",
            "step 5600: train ppl 3.75, val ppl 3.78\n",
            "step 5700: train loss 1.3187, val loss 1.3186\n",
            "step 5700: train ppl 3.75, val ppl 3.75\n",
            "step 5800: train loss 1.3290, val loss 1.3370\n",
            "step 5800: train ppl 3.79, val ppl 3.82\n",
            "step 5900: train loss 1.3180, val loss 1.3066\n",
            "step 5900: train ppl 3.75, val ppl 3.71\n",
            "step 6000: train loss 1.3152, val loss 1.3198\n",
            "step 6000: train ppl 3.74, val ppl 3.75\n",
            "step 6100: train loss 1.3156, val loss 1.3139\n",
            "step 6100: train ppl 3.74, val ppl 3.73\n",
            "step 6200: train loss 1.3056, val loss 1.3094\n",
            "step 6200: train ppl 3.70, val ppl 3.71\n",
            "step 6300: train loss 1.3114, val loss 1.3057\n",
            "step 6300: train ppl 3.72, val ppl 3.70\n",
            "step 6400: train loss 1.3118, val loss 1.3039\n",
            "step 6400: train ppl 3.72, val ppl 3.70\n",
            "step 6500: train loss 1.2985, val loss 1.3051\n",
            "step 6500: train ppl 3.67, val ppl 3.70\n",
            "step 6600: train loss 1.3075, val loss 1.2990\n",
            "step 6600: train ppl 3.71, val ppl 3.68\n",
            "step 6700: train loss 1.3140, val loss 1.3001\n",
            "step 6700: train ppl 3.73, val ppl 3.68\n",
            "step 6800: train loss 1.3020, val loss 1.2954\n",
            "step 6800: train ppl 3.69, val ppl 3.66\n",
            "step 6900: train loss 1.2911, val loss 1.2889\n",
            "step 6900: train ppl 3.65, val ppl 3.64\n",
            "step 7000: train loss 1.2861, val loss 1.2878\n",
            "step 7000: train ppl 3.63, val ppl 3.64\n",
            "step 7100: train loss 1.2955, val loss 1.2887\n",
            "step 7100: train ppl 3.67, val ppl 3.64\n",
            "step 7200: train loss 1.3000, val loss 1.2788\n",
            "step 7200: train ppl 3.68, val ppl 3.60\n",
            "step 7300: train loss 1.2961, val loss 1.2833\n",
            "step 7300: train ppl 3.67, val ppl 3.62\n",
            "step 7400: train loss 1.2935, val loss 1.2962\n",
            "step 7400: train ppl 3.66, val ppl 3.67\n",
            "step 7500: train loss 1.2827, val loss 1.2813\n",
            "step 7500: train ppl 3.62, val ppl 3.61\n",
            "step 7600: train loss 1.2803, val loss 1.2822\n",
            "step 7600: train ppl 3.61, val ppl 3.62\n",
            "step 7700: train loss 1.2861, val loss 1.2837\n",
            "step 7700: train ppl 3.63, val ppl 3.62\n",
            "step 7800: train loss 1.2768, val loss 1.2778\n",
            "step 7800: train ppl 3.60, val ppl 3.60\n",
            "step 7900: train loss 1.2769, val loss 1.2660\n",
            "step 7900: train ppl 3.60, val ppl 3.56\n",
            "step 8000: train loss 1.2721, val loss 1.2736\n",
            "step 8000: train ppl 3.58, val ppl 3.59\n",
            "step 8100: train loss 1.2713, val loss 1.2768\n",
            "step 8100: train ppl 3.58, val ppl 3.60\n",
            "step 8200: train loss 1.2732, val loss 1.2643\n",
            "step 8200: train ppl 3.58, val ppl 3.55\n",
            "step 8300: train loss 1.2631, val loss 1.2666\n",
            "step 8300: train ppl 3.55, val ppl 3.56\n",
            "step 8400: train loss 1.2736, val loss 1.2712\n",
            "step 8400: train ppl 3.59, val ppl 3.58\n",
            "step 8500: train loss 1.2692, val loss 1.2739\n",
            "step 8500: train ppl 3.57, val ppl 3.59\n",
            "step 8600: train loss 1.2613, val loss 1.2609\n",
            "step 8600: train ppl 3.54, val ppl 3.54\n",
            "step 8700: train loss 1.2604, val loss 1.2576\n",
            "step 8700: train ppl 3.54, val ppl 3.53\n",
            "step 8800: train loss 1.2659, val loss 1.2590\n",
            "step 8800: train ppl 3.56, val ppl 3.53\n",
            "step 8900: train loss 1.2583, val loss 1.2620\n",
            "step 8900: train ppl 3.53, val ppl 3.54\n",
            "step 9000: train loss 1.2601, val loss 1.2620\n",
            "step 9000: train ppl 3.54, val ppl 3.54\n",
            "step 9100: train loss 1.2546, val loss 1.2631\n",
            "step 9100: train ppl 3.52, val ppl 3.55\n",
            "step 9200: train loss 1.2566, val loss 1.2593\n",
            "step 9200: train ppl 3.52, val ppl 3.53\n",
            "step 9300: train loss 1.2553, val loss 1.2607\n",
            "step 9300: train ppl 3.52, val ppl 3.54\n",
            "step 9400: train loss 1.2542, val loss 1.2478\n",
            "step 9400: train ppl 3.51, val ppl 3.49\n",
            "step 9500: train loss 1.2593, val loss 1.2466\n",
            "step 9500: train ppl 3.53, val ppl 3.49\n",
            "step 9600: train loss 1.2525, val loss 1.2551\n",
            "step 9600: train ppl 3.51, val ppl 3.52\n",
            "step 9700: train loss 1.2491, val loss 1.2552\n",
            "step 9700: train ppl 3.50, val ppl 3.52\n",
            "step 9800: train loss 1.2432, val loss 1.2364\n",
            "step 9800: train ppl 3.48, val ppl 3.45\n",
            "step 9900: train loss 1.2545, val loss 1.2455\n",
            "step 9900: train ppl 3.52, val ppl 3.49\n"
          ]
        }
      ],
      "source": [
        "# 10. Instantiate model\n",
        "model = BigramLanguageModel().to(device)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# 11. Training loop\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']['loss']:.4f}, val loss {losses['val']['loss']:.4f}\")\n",
        "        print(f\"step {iter}: train ppl {losses['train']['perplexity']:.2f}, val ppl {losses['val']['perplexity']:.2f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7-LjqlFt0D7",
        "outputId": "baa8765f-0b5f-421a-d2f1-f83bd7c13eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\there. It's train again. So the er adventually. Everywhere you, Emmon.Once one day, Lily how Ben. Every and frastled and great in rack, she cand up. Mit was so harp her to the little mounce mean, she down. If things People langes acane to met started her doll. Then hugs the flower and flew away named Lily are very went's get about hand with her numbled and take her now and take it's place broker back and make a book nicel?\" Daisy got to \"My, make to the penny, he see they flew him away many helpider lemong and real your both brick, the king scared fun with her whenered the lessors. They played to called her graceful facen. It tide a drecided to help from decided the mair.Once upon a the sang. But It was oa too palace the land?\" \n",
            "\n",
            "The parents too. They say for being, Mar. Win to real incret hewa. When the ways him and prayed with and aftered! The nice was too long to Mom a could. Ben said,\n",
            "\n",
            "Mayally, Mom, it is not ing. The dog true after her her tade you can was so she could baby fight. It sees a smiled splash at works. It fox her mole called unding when he were over. The funfmered and a pennew Mom named Lily. They we make it of fun.\n",
            "\n",
            "On it was so3icy some nicely and splowed, and Of ducn a big to see she happy to the stand. One day he went on the animal. It that he seg. But the mexan was a back from all away. They lean to make him. She knew it was a big one things.Once a tiny, they felt her chewered for him.\n",
            "\n",
            "The farmer splactle, so ellown to play with their team plate. Pleaded for her he hose to hurt. He realliced on fun things in the lan said, goestle sing, but he had a fill, always long boreds with the wingher more that the faster laing and was can it the creus. The dog hurt. She sunthered to help the trow prature to old come. She went the place outh to the chew trow raing thing, the messy bunny. They saw thing her exciting pranes it picture and shiny.\n",
            "\n",
            "Sally sing together. When he heavy a shiny toys and day. Jy was so excited up to the towel nice. Canfin. One day we put she felt finned went to the carfuly scelending and herelep you back her at at her nas? Then, you listent. It can bu bed.Do you nelore was fun gumble. It preturers the obott down. Tom her the leaf.o Sharry nice happy to the pack and luggled and all he could if he was a big rrelage why were happy to hings for she hune?\"Yent says. His mom said we cryingted to be play ups.\n",
            "\n",
            "\n",
            "Lily who different. He packed around to pack the nesded some them. Made was she hanied, and they make need to help and giths ings. The friends. One day, a wake cheered herout. One day hello the merr. They are grass dring. Splaying, she wanted become twig up for the waterfy and her the shiny. From that day. Max were fun.\n",
            "\n",
            "\n",
            "The dog den, Lucy asked happy.\n",
            "\n",
            "They learned things to her after. Mia nods and staing find a friends friend, she shinbing the froce. The licme to beir.\n",
            "\n",
            "Sue coming buss.\n",
            "\n",
            "The spiciled becames to dinner. He allowards and the shohne. He was sowy that's it explained it lost.\n",
            "\n",
            "\"Ire have a littlef. We down't to the little trasted on the vendage. Cins!\" Sas a thought a from carrot. Sue started the back and fightreed it balance, and prat the ending again. Mamly red andwall he hear.\n",
            "\n",
            "And yhill day, happy and they day at the should rained on the day.\n",
            "\n",
            "One day, Sue can tlowed, please glass the spoons and he had bags, he wanted to take my tough it. It was so excine?\" \n",
            "\n",
            "Ben sleep liked on it the shinp away.\n",
            "\n",
            "He was so playing too pretty.Once upon a time there was a mabird with a meat. The sick, she limbed together. It had out. She felt sally no eatier.\n",
            "\n",
            "Alling and Molly.\n",
            " she ran to the painted.\n",
            "\n",
            "Emma many monwally and splashed, said. \n",
            "\n",
            "One day, there were that day on, but he was a very,\" Ben she shooks. Mayba relieved arman as cars.Once upon a tight!\n",
            "\n",
            "Mayons for his fuce pained and in its aways fort. The end, brancing around picture to help him he hair. The puppy special in the bothful boy back on a eng toy and Jack. It was a pecing if herson't kind melicies.\" Ben day and Mange felt.\n",
            "\n",
            "Lily Then you, the rone name. It's hand how to try, he sound.\n",
            "\n",
            "She smiled the matter lion and trus! \n",
            "\n",
            "Tained she was so hart for trough.\" She lut beed herstrues from go met it owander saw her and you drow him they held on the park pustround it not for the stay in a werow. They loved in of. Due wanted to herse help you down tree.\n",
            "\n",
            "The flower. Cholleve leanging. It find a want to play when shiny.Once upon a time, there was a little becamp why was so hing. They pack end.\n",
            "\n",
            "Then monkey smiled flowers.\n",
            "\n",
            "\"Go it of that the mars and brae adree that the closer. It feltherry friends. One day, Lily are friend.One day,\" he saidBye dog a tried, but Lily. With it wasn't eat.\n",
            "\n",
            "The storate friends be friends. The missed and said he had a new furion and the ideasor hingss Starter the nece. They lard a was refriend, Lily and Now! herstle.\n",
            "\n",
            "\n",
            "The saw a great always walreful fast.\n",
            "\n",
            "One day, Lily's friends in her their sad food up Momes the flower, and very proud of his and meats the bouse, kinds. The man w\n"
          ]
        }
      ],
      "source": [
        "# 12. Generate text\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=5000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model weights (PyTorch format)\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "\n",
        "# Save vocabulary and metadata (JSON format)\n",
        "data_to_save = {\n",
        "    'vocab': {\n",
        "        'stoi': stoi,  # Your string-to-index mapping\n",
        "        'itos': itos,  # Your index-to-string mapping\n",
        "    },\n",
        "    'metrics': {  # Optional training stats\n",
        "        'train_loss': losses['train']['loss'].item(),\n",
        "        'val_loss': losses['val']['loss'].item(),\n",
        "        'train_ppl': losses['train']['perplexity'].item(),\n",
        "        'val_ppl': losses['val']['perplexity'].item(),\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('model_vocab_metrics.json', 'w') as f:\n",
        "    json.dump(data_to_save, f, indent=4)\n",
        "\n",
        "print(\"Saved: model_weights.pth + model_vocab_metrics.json\")"
      ],
      "metadata": {
        "id": "WC82PLzFRUm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df354ad-e690-4a83-9332-9b9bab8d6eaa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: model_weights.pth + model_vocab_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== LOAD MODEL + VOCAB + METRICS ====================\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load model architecture\n",
        "model = BigramLanguageModel().to(device)\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "\n",
        "# Load vocabulary and training metrics\n",
        "with open('model_vocab_metrics.json', 'r') as f:\n",
        "    saved_data = json.load(f)\n",
        "\n",
        "# Extract vocab mappings\n",
        "stoi = saved_data['vocab']['stoi']\n",
        "itos = saved_data['vocab']['itos']\n",
        "\n",
        "# Define robust encode/decode functions\n",
        "def encode(s: str) -> list[int]:\n",
        "    \"\"\"Encode string to token IDs, skipping unknown characters.\"\"\"\n",
        "    return [stoi[c] for c in s if c in stoi]\n",
        "\n",
        "def decode(l: list[int]) -> str:\n",
        "    \"\"\"Decode token IDs to string, replacing unknown IDs with '<?>'.\"\"\"\n",
        "    return ''.join([itos.get(str(i), '<?>') for i in l])  # Ensure string key\n",
        "\n",
        "# Access training metrics\n",
        "metrics = saved_data.get('metrics', {})\n",
        "print(f\"Loaded model with vocab size {len(stoi)} | Val loss: {metrics.get('val_loss', 'N/A')}\")\n",
        "print(f\"First 5 vocab items: {dict(list(stoi.items())[:5])}\")\n",
        "\n",
        "# Test encoding and decoding\n",
        "test_text = ''.join([c for c in \"Hello Word\" if c in stoi])\n",
        "if test_text:\n",
        "    encoded = encode(test_text)\n",
        "    decoded = decode(encoded)\n",
        "    print(f\"Test encode/decode: '{test_text}' -> {encoded} -> '{decoded}'\")\n",
        "else:\n",
        "    print(\"Warning: Test text contains no known characters from vocabulary!\")\n"
      ],
      "metadata": {
        "id": "uBfCRoLxRfQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e11c79-b405-4db3-98d0-ab915e88d63b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model with vocab size 169 | Val loss: 1.245513677597046\n",
            "First 5 vocab items: {'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '\"': 4}\n",
            "Test encode/decode: 'Hello Word' -> [42, 70, 77, 77, 80, 2, 57, 80, 83, 69] -> 'Hello Word'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}